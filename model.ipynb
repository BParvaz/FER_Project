{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b98997",
   "metadata": {},
   "source": [
    "# Precursors\n",
    "The current iteration of this pipeline is coded to work only for FER2013  \n",
    "Plans include room for RAFDB, CK+ and other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad8f7",
   "metadata": {},
   "source": [
    "##### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be00c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.5.1\n",
      "Numpy version:  2.2.5\n",
      "Jupyter notebook version:  7.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ipykernel\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm # progress bar stuff, not necessary for a fully fleshed model\n",
    "import sys\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional \n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Any\n",
    "print(\"Pytorch version: \",torch.__version__)\n",
    "print(\"Numpy version: \",np.__version__)\n",
    "print(\"Jupyter notebook version: \",ipykernel.__version__)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e150d",
   "metadata": {},
   "source": [
    "#### 1. Import data and wrap around dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174feeeb",
   "metadata": {},
   "source": [
    "##### Create FERDataset Class to store datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4cf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FERDataset\n",
    "A class which inherits the pytorch dataset for storing and manipulating our input\n",
    "\"\"\"\n",
    "# Inherits Pytorch dataset\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform = None):\n",
    "        # Constructor\n",
    "        # (String) df: Instance of pandas dataframe with data already loaded\n",
    "        # (Function) transform: Image processing function \n",
    "        \n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        pixels = np.fromstring(row['pixels'], sep=' ', dtype=np.uint8)\n",
    "        img = pixels.reshape(48, 48) #48x48 to match FER2013\n",
    "\n",
    "        # from greyscale to RGB\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "\n",
    "        label = int(row['emotion'])\n",
    "\n",
    "        # transform img\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def get_emotion_class(self,emotion):\n",
    "        return FERDataset(self.df[self.df['emotion'] == emotion])\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        classes = self.df['emotion'].unique()\n",
    "        classes.sort()\n",
    "        return classes\n",
    "    \n",
    "    # Helper function to return distribution of each class in a dataset\n",
    "    @property\n",
    "    def dist(self):\n",
    "        return self.df['emotion'].value_counts()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673b52c",
   "metadata": {},
   "source": [
    "##### Load and test the dataset (FER2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549457af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fer2013_dataframe = pandas.read_csv('C:\\\\Users\\\\Bahram\\\\Desktop\\\\Personal Projects\\\\FER Tinkering\\\\From_Scratch\\\\data\\\\FER2013\\\\train.csv')\n",
    "dataset = FERDataset(dataframe=fer2013_dataframe)\n",
    "dataset.classes\n",
    "# Should be seven classes for FER2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8dbb2",
   "metadata": {},
   "source": [
    "##### Class dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6992bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Surprise\",\n",
    "    6: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa504e94",
   "metadata": {},
   "source": [
    "##### Grab an image and check its emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef9adf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAwADADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCXRoDb2pQHDyHe59a2ELBPl+761mWsoE7cYGKW78Q29gwgZZGJ5+VcigDVDE5DdajkMagbsGsq31yO/vJIoMnZGdwKkFT71mya9K0bBVhEkKjzDO+wDnHHqfYUAdKskfzbCQe4HpXL6vDaamrRSqN2Nu7HSm/8JFDEHWa7t1mWTYPLztbr8wb04x+IqhLeRyhpgw38uVzzj6UAXbe+aVGc5VSAVYI368dM01PD91ervjuZY33ZeSFFGfbLcjHtWxpc9vIwYqMAAAAdAO1bcKx7XEKZUn5snFAHO6bYSWd5sd3ZChMrsxzI3Y5rC+ywJr08b2yyKWygOeAev1rrb+9tbe7cXEgT5cgL7Vy9/rum3WowpCymRB1zgkZ/mKAJ9R8M287iZIojIRyRCAx+tZ0+gtAi/bM/MOdjYx9K6xruNtP82NiTj7pbI/CuW1C8uLmP7rYHAJoA09GBMkTEkmSMMxxjnv8ArXTJMYrdgn3icLmuVs9a026e2ltbqNmOR5W3awHU5HtW2JS6eZF8xQ5xQBn3vhOx1HUEurqSaQxqRt3EAk96zJvBWn2lyjwB1ON2WOT+fpWlLHqd4M3N59ihPISEZJ9y39BWXc2EJdpHvdQkfs3mdfwFAFuGOW1wpwyN6HNF7IhgCgDiqen2xivIjPeTFAeEcj8KpalfotzNgjCkgUAf/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=48x48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = dataset[123]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84dd77cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23463bee",
   "metadata": {},
   "source": [
    "### I don't think he looks angry, but moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971334a",
   "metadata": {},
   "source": [
    "##### 2. Transform each image and recreate the dataset using the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fbfe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48,48)),\n",
    "    transforms.ToTensor(),\n",
    "]) # lazy default transform\n",
    "dataset = FERDataset(fer2013_dataframe, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb7f1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 48, 48])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "image.shape # Three channels, 48x48 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880de4e",
   "metadata": {},
   "source": [
    "##### Dataloaders  \n",
    "For batching our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7c60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cfd07",
   "metadata": {},
   "source": [
    "At the end of this section, we have created the FERDataset class, which takes\n",
    "- A pandas dataframe\n",
    "- A preprocessing function for the data\n",
    "\n",
    "As a result, we can get\n",
    "- The length of the dataset\n",
    "- The next item in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb590530",
   "metadata": {},
   "source": [
    "# 2. Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fd19a",
   "metadata": {},
   "source": [
    "## [Using this as source](https://www.kaggle.com/code/galiasamuel/emotion-recognition-using-fer-dataset-resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8630ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hardware boilerplate\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa87e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Resnet\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "resnet_model = resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "438be056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3532150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to replace output layer with the number of classes our FER dataset has\n",
    "# Get the number of input features for the final fully connected layer\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "num_classes = 7 # no of classes in our data\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    # First hidden layer (Input: num_ftrs, Output: 512 nodes)\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), # Slightly higher dropout rate example\n",
    "\n",
    "    # Second hidden layer (Input: 512, Output: 256 nodes)\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), # Apply dropout again\n",
    "\n",
    "    # Output layer (Input: 256, Output: 7 classes)\n",
    "    nn.Linear(256, 7)\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "resnet_model.to(device)\n",
    "\n",
    "# Look at the final layer again\n",
    "resnet_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a71491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The kaggle model notes resne18's default transformations\n",
    "ResNet18_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e2a024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consts for mean and st.dev\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "TARGET_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bea7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code needs to be modified to deal with\n",
    "# Transforms for the training set (includes augmentation)\n",
    "train_transforms_resnet = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert 1-channel grayscale to 3 channels by repeating\n",
    "    transforms.Resize(256),                      # Resize the image (typically the shorter side) to 256\n",
    "    transforms.RandomCrop(TARGET_SIZE),          # Take a random 224x224 crop for data augmentation\n",
    "    transforms.RandomHorizontalFlip(),           # Standard data augmentation\n",
    "    transforms.RandomRotation(degrees=10),       # add slight rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2), # add brightness/contrast jitter\n",
    "    transforms.ToTensor(),                       # Convert PIL Image to PyTorch Tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) # Normalize with ImageNet values\n",
    "])\n",
    "\n",
    "# Transforms for the validation/testing set (deterministic preprocessing)\n",
    "val_transforms_resnet = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert 1-channel grayscale to 3 channels\n",
    "    transforms.Resize(256),                      # Resize the image (typically the shorter side) to 256\n",
    "    transforms.CenterCrop(TARGET_SIZE),          # Take the central 224x224 crop for evaluation\n",
    "    transforms.ToTensor(),                       # Convert PIL Image to PyTorch Tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) # Normalize with ImageNet values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224420fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3995 3995\n"
     ]
    }
   ],
   "source": [
    "# We can access the distribution of each emotion using our helper property dataset.dist\n",
    "dataset.dist\n",
    "val_size = (int(0.2*dataset.dist[0]))\n",
    "train_size = dataset.dist[0]-val_size\n",
    "print(dataset.dist[0],train_size+val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25287d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kaggle dataset uses a stratified split between training and validation\n",
    "# So I'll have to hack my way around creating my own version based on the kaggle code\n",
    "def Strat_Split(\n",
    "        dataset : FERDataset,\n",
    "        pct : float,\n",
    "        emotion : int\n",
    "        ):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    print(len(dataset))\n",
    "    # Define train and val dataset size\n",
    "    val_size = int(pct*dataset.dist[emotion])\n",
    "    train_size = dataset.dist[emotion]-val_size\n",
    "    train_dataset, val_dataset = random_split(dataset,[train_size,val_size],generator=generator)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42756603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry : 3995\n",
      "3995\n",
      "Disgust : 436\n",
      "436\n",
      "Fear : 4097\n",
      "4097\n",
      "Happy : 7215\n",
      "7215\n",
      "Sad : 4830\n",
      "4830\n",
      "Surprise : 3171\n",
      "3171\n",
      "Neutral : 4965\n",
      "4965\n"
     ]
    }
   ],
   "source": [
    "# He uses a \"ConcatDataset\" to join the disjoint datasets\n",
    "train_datasets = list()\n",
    "val_datasets = list()\n",
    "for emotion in dataset.classes:\n",
    "    # Create our stratified splits\n",
    "    dataset_emot = dataset.get_emotion_class(emotion)\n",
    "    print(emotion_names[emotion],\":\",len(dataset_emot))\n",
    "    train_emot, val_emot = Strat_Split(dataset_emot, 0.2, emotion)\n",
    "    # And append them together\n",
    "    train_datasets.append(train_emot)\n",
    "    val_datasets.append(val_emot)\n",
    "\n",
    "train = ConcatDataset(train_datasets)\n",
    "val = ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5631ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.transform = train_transforms_resnet\n",
    "val.transform = val_transforms_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3170cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2825bd9",
   "metadata": {},
   "source": [
    "#### FER2013 has been split into stratified sets, and now can be used with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c26ca6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22968\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4eb0df",
   "metadata": {},
   "source": [
    "### The next parts of the implementation of the model are directly from the kaggle codebase, I might tinker with it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b08b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device,\n",
    "               scheduler: torch.optim.lr_scheduler.OneCycleLR) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6. (added) Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# EDITED FOR PERFORMANCE AND TO IMPLEMENT EARLY STOPPING\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader, # Renamed from test_dataloader to val_dataloader for clarity with validation set\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          scheduler: torch.optim.lr_scheduler.OneCycleLR, # Type hint specific OneCycleLR as used\n",
    "          device: torch.device,\n",
    "          # --- Early Stopping Parameters ---\n",
    "          patience: int = 10, # How many epochs to wait for improvement before stopping\n",
    "          min_delta: float = 0.0 # Minimum change to qualify as improvement\n",
    "          # ---------------------------------\n",
    "         ) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model with Early Stopping.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop. Stops early if validation performance\n",
    "    doesn't improve for 'patience' epochs.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "    Saves the best model state based on validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained and tested.\n",
    "        train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "        test_dataloader: A DataLoader instance for the model to be validated on.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "        loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "        epochs: An integer indicating how many epochs to train for (maximum).\n",
    "        scheduler: A PyTorch learning rate scheduler (assumed to be stepped per batch).\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "        patience: Number of epochs to wait for improvement in validation accuracy.\n",
    "        min_delta: Minimum change in validation accuracy to be considered an improvement.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics tracked per epoch. The input 'model' object\n",
    "        will be updated in-place with the best weights found during training\n",
    "        (based on validation accuracy).\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [], # Renamed from test_loss to val_loss\n",
    "               \"test_acc\": []  # Renamed from test_acc to val_acc\n",
    "    }\n",
    "\n",
    "    # --- Early Stopping Initialization ---\n",
    "    best_val_accuracy = -float('inf') # Initialize best accuracy to a very low value\n",
    "    patience_counter = 0             # Initialize patience counter\n",
    "    best_model_state_dict = None     # To store the state_dict of the best model\n",
    "    print(f\"Early stopping configured with patience={patience}, min_delta={min_delta}\")\n",
    "    # ------------------------------------\n",
    "\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Assuming train_step includes scheduler.step() per batch\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                             dataloader=train_dataloader,\n",
    "                                             loss_fn=loss_fn,\n",
    "                                             optimizer=optimizer,\n",
    "                                             device=device,\n",
    "                                             scheduler=scheduler) # Pass scheduler\n",
    "\n",
    "        # Assuming test_step is the validation step\n",
    "        val_loss, val_acc = test_step(model=model, # Renamed test_step calls to clarify validation\n",
    "                                          dataloader=test_dataloader, # Using test_dataloader as val_dataloader\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \" # Renamed print output\n",
    "            f\"val_acc: {val_acc:.4f}\"     # Renamed print output\n",
    "        )\n",
    "\n",
    "        # --- Early Stopping Logic ---\n",
    "        # Check for improvement in validation accuracy\n",
    "        if val_acc > best_val_accuracy + min_delta:\n",
    "            best_val_accuracy = val_acc\n",
    "            patience_counter = 0 # Reset patience counter on improvement\n",
    "            # Save the model state dictionary\n",
    "            # Use copy.deepcopy() to save a true copy, not just a reference\n",
    "            best_model_state_dict = deepcopy(model.state_dict())\n",
    "            print(f\"  Validation accuracy improved. Saving best model state (Acc: {best_val_accuracy:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1 # Increment patience counter if no improvement\n",
    "            print(f\"  Validation accuracy did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break # Exit the training loop\n",
    "        \"\"\"\n",
    "        we dont use wandb\n",
    "        # --- End Early Stopping Logic ---\n",
    "        # Log metrics to wandb\n",
    "        run.log({\"epoch\": epoch+1,\n",
    "               \"train_loss\": train_loss,\n",
    "               \"train_acc\": train_acc,\n",
    "               \"test_loss\": val_loss,\n",
    "               \"test_acc\": val_acc})\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(val_loss) # Store as test_loss in results dict for consistency if needed later\n",
    "        results[\"test_acc\"].append(val_acc)   # Store as test_acc in results dict\n",
    "\n",
    "\n",
    "    # --- Load the best model state after training stops ---\n",
    "    if best_model_state_dict is not None:\n",
    "        print(\"Loading best model state found during training.\")\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "    else:\n",
    "         print(\"No improvement was found during training. Using the model from the last epoch.\")\n",
    "\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f7c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastai.vision.all import *\n",
    "# defining the DataLoaders object in FastAI using both train and val datasets\n",
    "dls = DataLoaders(train_dataloader, val_dataloader)\n",
    "\n",
    "# wrapping our dataloader in a fastai learner\n",
    "learn = Learner(\n",
    "    dls, resnet_model,\n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    opt_func=SGD,\n",
    "    metrics=accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411415cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='718' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/718 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finding the learning rate\n",
    "lr_result = learn.lr_find()\n",
    "print(lr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11c636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Self_Model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
