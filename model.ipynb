{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b98997",
   "metadata": {},
   "source": [
    "# Precursors\n",
    "The current iteration of this pipeline is coded to work only for FER2013  \n",
    "Plans include room for RAFDB, CK+ and other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad8f7",
   "metadata": {},
   "source": [
    "##### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "be00c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.5.1\n",
      "Numpy version:  2.2.5\n",
      "Jupyter notebook version:  7.1.0\n",
      "Timm version:  1.0.22\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ipykernel\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import timm\n",
    "print(\"Pytorch version: \",torch.__version__)\n",
    "print(\"Numpy version: \",np.__version__)\n",
    "print(\"Jupyter notebook version: \",ipykernel.__version__)\n",
    "print(\"Timm version: \",timm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e150d",
   "metadata": {},
   "source": [
    "#### 1. Import data and wrap around dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174feeeb",
   "metadata": {},
   "source": [
    "##### Create FERDataset Class to store datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FERDataset\n",
    "A class which inherits the pytorch dataset for storing and manipulating our input\n",
    "\"\"\"\n",
    "# Inherits Pytorch dataset\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform = None):\n",
    "        # Constructor\n",
    "        # (String) df: Instance of pandas dataframe with data already loaded\n",
    "        # (Function) transform: Image processing function \n",
    "        \n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        pixels = np.fromstring(row['pixels'], sep=' ', dtype=np.uint8)\n",
    "        img = pixels.reshape(48, 48) #48x48 to match FER2013\n",
    "\n",
    "        # from greyscale to RGB\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "\n",
    "        label = int(row['emotion'])\n",
    "\n",
    "        # transform img\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        classes = self.df['emotion'].unique()\n",
    "        classes.sort()\n",
    "        return classes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673b52c",
   "metadata": {},
   "source": [
    "##### Load and test the dataset (FER2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "549457af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fer2013_dataframe = pandas.read_csv('C:\\\\Users\\\\Bahram\\\\Desktop\\\\Personal Projects\\\\FER Tinkering\\\\From_Scratch\\\\data\\\\FER2013\\\\fer2013.csv')\n",
    "dataset = FERDataset(dataframe=fer2013_dataframe)\n",
    "dataset.classes\n",
    "# Should be seven classes for FER2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8dbb2",
   "metadata": {},
   "source": [
    "##### Class dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e6992bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Surprise\",\n",
    "    6: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa504e94",
   "metadata": {},
   "source": [
    "##### Grab an image and check its emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fef9adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset[123]\n",
    "#image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd77cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23463bee",
   "metadata": {},
   "source": [
    "### I don't think he looks angry, but moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971334a",
   "metadata": {},
   "source": [
    "##### 2. Transform each image and recreate the dataset using the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fbfe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48,48)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = FERDataset(fer2013_dataframe, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7f1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 48, 48])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "image.shape # Three channels, 48x48 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880de4e",
   "metadata": {},
   "source": [
    "##### Dataloaders  \n",
    "For batching our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c7c60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset,batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Self_Model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
