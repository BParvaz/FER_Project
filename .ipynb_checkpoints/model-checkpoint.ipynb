{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b98997",
   "metadata": {},
   "source": [
    "# Precursors\n",
    "The current iteration of this pipeline is coded to work only for FER2013  \n",
    "Plans include room for RAFDB, CK+ and other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad8f7",
   "metadata": {},
   "source": [
    "##### Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be00c961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/iusers01/fse-ugpgt01/compsci01/b84547bp/.conda/envs/Self_Model/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/mnt/iusers01/fse-ugpgt01/compsci01/b84547bp/.conda/envs/Self_Model/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.6.0\n",
      "Numpy version:  1.26.4\n",
      "Jupyter notebook version:  6.31.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ipykernel\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm # progress bar stuff, not necessary for a fully fleshed model\n",
    "import sys\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional \n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Any\n",
    "print(\"Pytorch version: \",torch.__version__)\n",
    "print(\"Numpy version: \",np.__version__)\n",
    "print(\"Jupyter notebook version: \",ipykernel.__version__)\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e150d",
   "metadata": {},
   "source": [
    "#### 1. Import data and wrap around dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174feeeb",
   "metadata": {},
   "source": [
    "##### Create FERDataset Class to store datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4cf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FERDataset\n",
    "A class which inherits the pytorch dataset for storing and manipulating our input\n",
    "\"\"\"\n",
    "# Inherits Pytorch dataset\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform = None):\n",
    "        # Constructor\n",
    "        # (String) df: Instance of pandas dataframe with data already loaded\n",
    "        # (Function) transform: Image processing function \n",
    "        \n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        pixels = np.fromstring(row['pixels'], sep=' ', dtype=np.uint8)\n",
    "        img = pixels.reshape(48, 48) #48x48 to match FER2013\n",
    "\n",
    "        # from greyscale to RGB\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "\n",
    "        label = int(row['emotion'])\n",
    "\n",
    "        # transform img\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def get_emotion_class(self,emotion):\n",
    "        return FERDataset(self.df[self.df['emotion'] == emotion])\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        classes = self.df['emotion'].unique()\n",
    "        classes.sort()\n",
    "        return classes\n",
    "    \n",
    "    # Helper function to return distribution of each class in a dataset\n",
    "    @property\n",
    "    def dist(self):\n",
    "        return self.df['emotion'].value_counts()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673b52c",
   "metadata": {},
   "source": [
    "##### Load and test the dataset (FER2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549457af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Bahram\\\\Desktop\\\\Personal Projects\\\\FER Tinkering\\\\From_Scratch\\\\data\\\\FER2013\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fer2013_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mBahram\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mPersonal Projects\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mFER Tinkering\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mFrom_Scratch\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mFER2013\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m FERDataset(dataframe\u001b[38;5;241m=\u001b[39mfer2013_dataframe)\n\u001b[1;32m      3\u001b[0m dataset\u001b[38;5;241m.\u001b[39mclasses\n",
      "File \u001b[0;32m~/.conda/envs/Self_Model/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/Self_Model/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/Self_Model/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/Self_Model/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/Self_Model/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Bahram\\\\Desktop\\\\Personal Projects\\\\FER Tinkering\\\\From_Scratch\\\\data\\\\FER2013\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "fer2013_dataframe = pandas.read_csv('~\\\\data\\\\FER2013\\\\train.csv')\n",
    "dataset = FERDataset(dataframe=fer2013_dataframe)\n",
    "dataset.classes\n",
    "# Should be seven classes for FER2013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8dbb2",
   "metadata": {},
   "source": [
    "##### Class dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6992bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Surprise\",\n",
    "    6: \"Neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa504e94",
   "metadata": {},
   "source": [
    "##### Grab an image and check its emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset[123]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd77cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23463bee",
   "metadata": {},
   "source": [
    "### I don't think he looks angry, but moving on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971334a",
   "metadata": {},
   "source": [
    "##### 2. Transform each image and recreate the dataset using the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfe249",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48,48)),\n",
    "    transforms.ToTensor(),\n",
    "]) # lazy default transform\n",
    "dataset = FERDataset(fer2013_dataframe, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset[0]\n",
    "image.shape # Three channels, 48x48 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880de4e",
   "metadata": {},
   "source": [
    "##### Dataloaders  \n",
    "For batching our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734cfd07",
   "metadata": {},
   "source": [
    "At the end of this section, we have created the FERDataset class, which takes\n",
    "- A pandas dataframe\n",
    "- A preprocessing function for the data\n",
    "\n",
    "As a result, we can get\n",
    "- The length of the dataset\n",
    "- The next item in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb590530",
   "metadata": {},
   "source": [
    "# 2. Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fd19a",
   "metadata": {},
   "source": [
    "## [Using this as source](https://www.kaggle.com/code/galiasamuel/emotion-recognition-using-fer-dataset-resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware boilerplate\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa87e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Resnet\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "resnet_model = resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438be056",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to replace output layer with the number of classes our FER dataset has\n",
    "# Get the number of input features for the final fully connected layer\n",
    "num_ftrs = resnet_model.fc.in_features\n",
    "num_classes = 7 # no of classes in our data\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    # First hidden layer (Input: num_ftrs, Output: 512 nodes)\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), # Slightly higher dropout rate example\n",
    "\n",
    "    # Second hidden layer (Input: 512, Output: 256 nodes)\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5), # Apply dropout again\n",
    "\n",
    "    # Output layer (Input: 256, Output: 7 classes)\n",
    "    nn.Linear(256, 7)\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "resnet_model.to(device)\n",
    "\n",
    "# Look at the final layer again\n",
    "resnet_model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kaggle model notes resne18's default transformations\n",
    "ResNet18_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consts for mean and st.dev\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "TARGET_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea7426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code needs to be modified to deal with\n",
    "# Transforms for the training set (includes augmentation)\n",
    "train_transforms_resnet = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert 1-channel grayscale to 3 channels by repeating\n",
    "    transforms.Resize(256),                      # Resize the image (typically the shorter side) to 256\n",
    "    transforms.RandomCrop(TARGET_SIZE),          # Take a random 224x224 crop for data augmentation\n",
    "    transforms.RandomHorizontalFlip(),           # Standard data augmentation\n",
    "    transforms.RandomRotation(degrees=10),       # add slight rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2), # add brightness/contrast jitter\n",
    "    transforms.ToTensor(),                       # Convert PIL Image to PyTorch Tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) # Normalize with ImageNet values\n",
    "])\n",
    "\n",
    "# Transforms for the validation/testing set (deterministic preprocessing)\n",
    "val_transforms_resnet = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert 1-channel grayscale to 3 channels\n",
    "    transforms.Resize(256),                      # Resize the image (typically the shorter side) to 256\n",
    "    transforms.CenterCrop(TARGET_SIZE),          # Take the central 224x224 crop for evaluation\n",
    "    transforms.ToTensor(),                       # Convert PIL Image to PyTorch Tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD) # Normalize with ImageNet values\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224420fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the distribution of each emotion using our helper property dataset.dist\n",
    "dataset.dist\n",
    "val_size = (int(0.2*dataset.dist[0]))\n",
    "train_size = dataset.dist[0]-val_size\n",
    "print(dataset.dist[0],train_size+val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25287d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kaggle dataset uses a stratified split between training and validation\n",
    "# So I'll have to hack my way around creating my own version based on the kaggle code\n",
    "def Strat_Split(\n",
    "        dataset : FERDataset,\n",
    "        pct : float,\n",
    "        emotion : int\n",
    "        ):\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    print(len(dataset))\n",
    "    # Define train and val dataset size\n",
    "    val_size = int(pct*dataset.dist[emotion])\n",
    "    train_size = dataset.dist[emotion]-val_size\n",
    "    train_dataset, val_dataset = random_split(dataset,[train_size,val_size],generator=generator)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42756603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# He uses a \"ConcatDataset\" to join the disjoint datasets\n",
    "train_datasets = list()\n",
    "val_datasets = list()\n",
    "for emotion in dataset.classes:\n",
    "    # Create our stratified splits\n",
    "    dataset_emot = dataset.get_emotion_class(emotion)\n",
    "    print(emotion_names[emotion],\":\",len(dataset_emot))\n",
    "    train_emot, val_emot = Strat_Split(dataset_emot, 0.2, emotion)\n",
    "    # And append them together\n",
    "    train_datasets.append(train_emot)\n",
    "    val_datasets.append(val_emot)\n",
    "\n",
    "train = ConcatDataset(train_datasets)\n",
    "val = ConcatDataset(val_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.transform = train_transforms_resnet\n",
    "val.transform = val_transforms_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2825bd9",
   "metadata": {},
   "source": [
    "#### FER2013 has been split into stratified sets, and now can be used with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ca6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4eb0df",
   "metadata": {},
   "source": [
    "### The next parts of the implementation of the model are directly from the kaggle codebase, I might tinker with it in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b08b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device,\n",
    "               scheduler: torch.optim.lr_scheduler.OneCycleLR) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "    \"\"\"\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6. (added) Scheduler step\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# EDITED FOR PERFORMANCE AND TO IMPLEMENT EARLY STOPPING\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader, # Renamed from test_dataloader to val_dataloader for clarity with validation set\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          scheduler: torch.optim.lr_scheduler.OneCycleLR, # Type hint specific OneCycleLR as used\n",
    "          device: torch.device,\n",
    "          # --- Early Stopping Parameters ---\n",
    "          patience: int = 10, # How many epochs to wait for improvement before stopping\n",
    "          min_delta: float = 0.0 # Minimum change to qualify as improvement\n",
    "          # ---------------------------------\n",
    "         ) -> Dict[str, List[float]]:\n",
    "    \"\"\"Trains and tests a PyTorch model with Early Stopping.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop. Stops early if validation performance\n",
    "    doesn't improve for 'patience' epochs.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "    Saves the best model state based on validation accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained and tested.\n",
    "        train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "        test_dataloader: A DataLoader instance for the model to be validated on.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "        loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "        epochs: An integer indicating how many epochs to train for (maximum).\n",
    "        scheduler: A PyTorch learning rate scheduler (assumed to be stepped per batch).\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "        patience: Number of epochs to wait for improvement in validation accuracy.\n",
    "        min_delta: Minimum change in validation accuracy to be considered an improvement.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics tracked per epoch. The input 'model' object\n",
    "        will be updated in-place with the best weights found during training\n",
    "        (based on validation accuracy).\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [], # Renamed from test_loss to val_loss\n",
    "               \"test_acc\": []  # Renamed from test_acc to val_acc\n",
    "    }\n",
    "\n",
    "    # --- Early Stopping Initialization ---\n",
    "    best_val_accuracy = -float('inf') # Initialize best accuracy to a very low value\n",
    "    patience_counter = 0             # Initialize patience counter\n",
    "    best_model_state_dict = None     # To store the state_dict of the best model\n",
    "    print(f\"Early stopping configured with patience={patience}, min_delta={min_delta}\")\n",
    "    # ------------------------------------\n",
    "\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Assuming train_step includes scheduler.step() per batch\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                             dataloader=train_dataloader,\n",
    "                                             loss_fn=loss_fn,\n",
    "                                             optimizer=optimizer,\n",
    "                                             device=device,\n",
    "                                             scheduler=scheduler) # Pass scheduler\n",
    "\n",
    "        # Assuming test_step is the validation step\n",
    "        val_loss, val_acc = test_step(model=model, # Renamed test_step calls to clarify validation\n",
    "                                          dataloader=test_dataloader, # Using test_dataloader as val_dataloader\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"val_loss: {val_loss:.4f} | \" # Renamed print output\n",
    "            f\"val_acc: {val_acc:.4f}\"     # Renamed print output\n",
    "        )\n",
    "\n",
    "        # --- Early Stopping Logic ---\n",
    "        # Check for improvement in validation accuracy\n",
    "        if val_acc > best_val_accuracy + min_delta:\n",
    "            best_val_accuracy = val_acc\n",
    "            patience_counter = 0 # Reset patience counter on improvement\n",
    "            # Save the model state dictionary\n",
    "            # Use copy.deepcopy() to save a true copy, not just a reference\n",
    "            best_model_state_dict = deepcopy(model.state_dict())\n",
    "            print(f\"  Validation accuracy improved. Saving best model state (Acc: {best_val_accuracy:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1 # Increment patience counter if no improvement\n",
    "            print(f\"  Validation accuracy did not improve. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break # Exit the training loop\n",
    "        \"\"\"\n",
    "        we dont use wandb\n",
    "        # --- End Early Stopping Logic ---\n",
    "        # Log metrics to wandb\n",
    "        run.log({\"epoch\": epoch+1,\n",
    "               \"train_loss\": train_loss,\n",
    "               \"train_acc\": train_acc,\n",
    "               \"test_loss\": val_loss,\n",
    "               \"test_acc\": val_acc})\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(val_loss) # Store as test_loss in results dict for consistency if needed later\n",
    "        results[\"test_acc\"].append(val_acc)   # Store as test_acc in results dict\n",
    "\n",
    "\n",
    "    # --- Load the best model state after training stops ---\n",
    "    if best_model_state_dict is not None:\n",
    "        print(\"Loading best model state found during training.\")\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "    else:\n",
    "         print(\"No improvement was found during training. Using the model from the last epoch.\")\n",
    "\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastai.vision.all import *\n",
    "# defining the DataLoaders object in FastAI using both train and val datasets\n",
    "dls = DataLoaders(train_dataloader, val_dataloader)\n",
    "\n",
    "# wrapping our dataloader in a fastai learner\n",
    "learn = Learner(\n",
    "    dls, resnet_model,\n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    opt_func=SGD,\n",
    "    metrics=accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the learning rate\n",
    "lr_result = learn.lr_find()\n",
    "print(lr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11c636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1cbcb-d4f2-4be3-87a6-9030b07d5449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (Self_Model)",
   "language": "python",
   "name": "self_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
